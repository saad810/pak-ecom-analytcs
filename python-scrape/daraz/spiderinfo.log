2025-09-12 18:13:42 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:13:42 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:13:42 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:13:42 [scrapy.extensions.telnet] INFO: Telnet Password: a159e951ce7bf272
2025-09-12 18:13:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:13:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'HTTPCACHE_ENABLED': True,
 'HTTPCACHE_EXPIRATION_SECS': 86400,
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:13:43 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:13:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats',
 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']
2025-09-12 18:13:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:13:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:13:43 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:13:43 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:13:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:13:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:13:43 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:13:43 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:13:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.daraz.pk/i134869708-s1295172171.html>: HTTP status code is not handled or not allowed
2025-09-12 18:13:45 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:13:45 [scrapy.extensions.feedexport] INFO: Stored json feed (4 items) in: data/prod5-5-w-selenium.json
2025-09-12 18:13:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1543,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 2759697,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 2.318352,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 13, 45, 580258, tzinfo=datetime.timezone.utc),
 'httpcache/hit': 6,
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'item_scraped_count': 4,
 'items_per_minute': 120.0,
 'log_count/INFO': 15,
 'log_count/WARNING': 1,
 'response_received_count': 6,
 'responses_per_minute': 180.0,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'start_time': datetime.datetime(2025, 9, 12, 13, 13, 43, 261906, tzinfo=datetime.timezone.utc)}
2025-09-12 18:13:45 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:13:45 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:13:45 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:15:31 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:15:31 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:15:31 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:15:31 [scrapy.extensions.telnet] INFO: Telnet Password: 1c3524726da79bf0
2025-09-12 18:15:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:15:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:15:31 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:15:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:15:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:15:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:15:32 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:15:32 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:15:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:15:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:15:32 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:15:32 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:15:33 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:15:34 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:15:52 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:15:53 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:15:53 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-09-12 18:15:53 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-09-12 18:15:53 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i134944346-s1295276893.html> exc_type=<class 'Exception'> exc_msg=Page.goto: Connection closed while reading from the driver
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2025-09-12 18:15:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i134944346-s1295276893.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 450, in _download_request_with_retry
    await page.close()
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 9794, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 839, in close
    raise e
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 834, in close
    await self._channel.send("close", None, locals_to_params(locals()))
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2025-09-12 18:15:54 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-09-12 18:15:54 [asyncio] ERROR: Task was destroyed but it is pending!
task: <Task pending name='Task-16' coro=<_ThreadedLoopAdapter._handle_coro() running at D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py:123> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[ProtocolCallback.__init__.<locals>.cb() at D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py:228, ProtocolCallback.__init__.<locals>.cb() at D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py:228]>
2025-09-12 18:15:54 [asyncio] ERROR: Task was destroyed but it is pending!
task: <Task pending name='Task-24' coro=<_ThreadedLoopAdapter._handle_coro() running at D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py:123> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-12 18:15:54 [asyncio] ERROR: Task was destroyed but it is pending!
task: <Task pending name='Task-26' coro=<_ThreadedLoopAdapter._handle_coro() running at D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py:123> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-12 18:16:01 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:16:01 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:16:01 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:16:01 [scrapy.extensions.telnet] INFO: Telnet Password: f4cd917f6f8970f0
2025-09-12 18:16:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:16:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:16:02 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:16:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:16:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:16:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:16:02 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:16:02 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:16:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:16:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:16:02 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:16:02 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:16:04 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:16:04 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:17:01 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:17:01 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:17:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1423,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 2894087,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'elapsed_time_seconds': 59.047413,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 17, 1, 345653, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 5,
 'items_per_minute': 5.084745762711865,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 5,
 'playwright/page_count/closed': 5,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 840,
 'playwright/request_count/method/GET': 727,
 'playwright/request_count/method/POST': 113,
 'playwright/request_count/navigation': 25,
 'playwright/request_count/resource_type/document': 25,
 'playwright/request_count/resource_type/fetch': 20,
 'playwright/request_count/resource_type/font': 30,
 'playwright/request_count/resource_type/image': 287,
 'playwright/request_count/resource_type/ping': 83,
 'playwright/request_count/resource_type/script': 309,
 'playwright/request_count/resource_type/stylesheet': 40,
 'playwright/request_count/resource_type/xhr': 46,
 'playwright/response_count': 796,
 'playwright/response_count/method/GET': 704,
 'playwright/response_count/method/POST': 92,
 'playwright/response_count/resource_type/document': 25,
 'playwright/response_count/resource_type/fetch': 20,
 'playwright/response_count/resource_type/font': 25,
 'playwright/response_count/resource_type/image': 284,
 'playwright/response_count/resource_type/ping': 63,
 'playwright/response_count/resource_type/script': 295,
 'playwright/response_count/resource_type/stylesheet': 39,
 'playwright/response_count/resource_type/xhr': 45,
 'response_received_count': 6,
 'responses_per_minute': 6.101694915254238,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'start_time': datetime.datetime(2025, 9, 12, 13, 16, 2, 298240, tzinfo=datetime.timezone.utc)}
2025-09-12 18:17:01 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:17:01 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:17:01 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:17:01 [scrapy-playwright] INFO: Closing browser
2025-09-12 18:19:48 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:19:48 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:19:48 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:19:48 [scrapy.extensions.telnet] INFO: Telnet Password: 2150bd2355ee9a7f
2025-09-12 18:19:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:19:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:19:49 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:19:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:19:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:19:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:19:49 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:19:49 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:19:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:19:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:19:49 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:19:49 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:19:50 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:19:51 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:20:44 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:20:44 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:20:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1155296,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 55.135398,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 20, 44, 319457, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 2.181818181818182,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 359,
 'playwright/request_count/method/GET': 306,
 'playwright/request_count/method/POST': 53,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 13,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 39,
 'playwright/request_count/resource_type/script': 126,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 341,
 'playwright/response_count/method/GET': 294,
 'playwright/response_count/method/POST': 47,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 13,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 117,
 'playwright/response_count/resource_type/ping': 33,
 'playwright/response_count/resource_type/script': 120,
 'playwright/response_count/resource_type/stylesheet': 14,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 3.272727272727273,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 13, 19, 49, 184059, tzinfo=datetime.timezone.utc)}
2025-09-12 18:20:44 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:20:44 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:20:44 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:20:44 [scrapy-playwright] INFO: Closing browser
2025-09-12 18:26:36 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:26:36 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:26:36 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:26:36 [scrapy.extensions.telnet] INFO: Telnet Password: 63434d660c851a24
2025-09-12 18:26:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:26:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:26:36 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:26:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:26:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:26:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:26:37 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:26:37 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:26:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:26:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:26:37 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:26:37 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:26:38 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:26:39 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:26:59 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:26:59 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:26:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157749,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 22.565641,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 26, 59, 655181, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 5.454545454545455,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 350,
 'playwright/request_count/method/GET': 306,
 'playwright/request_count/method/POST': 44,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 10,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 31,
 'playwright/request_count/resource_type/script': 127,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 21,
 'playwright/response_count': 334,
 'playwright/response_count/method/GET': 297,
 'playwright/response_count/method/POST': 37,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 10,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 120,
 'playwright/response_count/resource_type/ping': 24,
 'playwright/response_count/resource_type/script': 120,
 'playwright/response_count/resource_type/stylesheet': 15,
 'playwright/response_count/resource_type/xhr': 21,
 'response_received_count': 3,
 'responses_per_minute': 8.181818181818182,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 13, 26, 37, 89540, tzinfo=datetime.timezone.utc)}
2025-09-12 18:26:59 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:26:59 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:26:59 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:26:59 [scrapy-playwright] INFO: Closing browser
2025-09-12 18:28:28 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:28:28 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:28:28 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:28:28 [scrapy.extensions.telnet] INFO: Telnet Password: 7dc2b37bb547f6de
2025-09-12 18:28:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:28:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:28:28 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:28:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:28:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:28:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:28:28 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:28:28 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:28:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:28:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:28:28 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:28:28 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:28:30 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:28:30 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:28:52 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:28:52 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:28:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157549,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 23.174433,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 28, 52, 28486, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 5.217391304347826,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 354,
 'playwright/request_count/method/GET': 305,
 'playwright/request_count/method/POST': 49,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 7,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 119,
 'playwright/request_count/resource_type/ping': 39,
 'playwright/request_count/resource_type/script': 128,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 337,
 'playwright/response_count/method/GET': 295,
 'playwright/response_count/method/POST': 42,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 7,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 119,
 'playwright/response_count/resource_type/ping': 32,
 'playwright/response_count/resource_type/script': 121,
 'playwright/response_count/resource_type/stylesheet': 14,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 7.826086956521738,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 13, 28, 28, 854053, tzinfo=datetime.timezone.utc)}
2025-09-12 18:28:52 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:28:52 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:28:52 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:28:52 [scrapy-playwright] INFO: Closing browser
2025-09-12 18:29:37 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:29:37 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:29:37 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:29:37 [scrapy.extensions.telnet] INFO: Telnet Password: 4bcf87bf1487abbf
2025-09-12 18:29:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:29:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:29:38 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:29:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:29:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:29:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:29:38 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:29:38 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:29:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:29:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:29:38 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:29:38 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:29:40 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:29:41 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:29:59 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:29:59 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:29:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1155148,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 21.292862,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 29, 59, 771355, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 5.714285714285714,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 350,
 'playwright/request_count/method/GET': 304,
 'playwright/request_count/method/POST': 46,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 7,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 119,
 'playwright/request_count/resource_type/ping': 36,
 'playwright/request_count/resource_type/script': 127,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 332,
 'playwright/response_count/method/GET': 296,
 'playwright/response_count/method/POST': 36,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 7,
 'playwright/response_count/resource_type/font': 12,
 'playwright/response_count/resource_type/image': 119,
 'playwright/response_count/resource_type/ping': 26,
 'playwright/response_count/resource_type/script': 120,
 'playwright/response_count/resource_type/stylesheet': 15,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 8.571428571428571,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 13, 29, 38, 478493, tzinfo=datetime.timezone.utc)}
2025-09-12 18:29:59 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:29:59 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:29:59 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:30:00 [scrapy-playwright] INFO: Closing browser
2025-09-12 18:38:59 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:39:00 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:39:00 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:39:00 [scrapy.extensions.telnet] INFO: Telnet Password: 904be46fe71776c0
2025-09-12 18:39:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:39:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:39:00 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:39:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:39:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:39:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:39:01 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:39:01 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:39:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:39:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:39:01 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:39:01 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:39:03 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:39:04 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:40:01 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-09-12 18:40:10 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:40:10 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:40:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1155330,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 69.479034,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 40, 10, 808505, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 1.7391304347826089,
 'log_count/INFO': 17,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 356,
 'playwright/request_count/method/GET': 306,
 'playwright/request_count/method/POST': 50,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 11,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 37,
 'playwright/request_count/resource_type/script': 127,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 338,
 'playwright/response_count/method/GET': 295,
 'playwright/response_count/method/POST': 43,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 11,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 117,
 'playwright/response_count/resource_type/ping': 30,
 'playwright/response_count/resource_type/script': 121,
 'playwright/response_count/resource_type/stylesheet': 15,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 2.608695652173913,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 13, 39, 1, 329471, tzinfo=datetime.timezone.utc)}
2025-09-12 18:40:10 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:40:10 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:40:10 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:40:10 [scrapy-playwright] INFO: Closing browser
2025-09-12 18:40:42 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 18:40:42 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 18:40:42 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 18:40:42 [scrapy.extensions.telnet] INFO: Telnet Password: 28ea482ad173ef03
2025-09-12 18:40:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 18:40:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 18:40:43 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 18:40:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 18:40:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 18:40:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 18:40:43 [scrapy.core.engine] INFO: Spider opened
2025-09-12 18:40:43 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 18:40:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 18:40:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 18:40:43 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:40:43 [scrapy-playwright] INFO: Starting download handler
2025-09-12 18:40:45 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 18:40:45 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 18:41:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 18:41:36 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-6-w-selenium.json
2025-09-12 18:41:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157561,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 53.166197,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 13, 41, 36, 575668, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 2.2641509433962264,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 357,
 'playwright/request_count/method/GET': 304,
 'playwright/request_count/method/POST': 53,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 11,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 39,
 'playwright/request_count/resource_type/script': 126,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 341,
 'playwright/response_count/method/GET': 296,
 'playwright/response_count/method/POST': 45,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 9,
 'playwright/response_count/resource_type/font': 12,
 'playwright/response_count/resource_type/image': 120,
 'playwright/response_count/resource_type/ping': 33,
 'playwright/response_count/resource_type/script': 119,
 'playwright/response_count/resource_type/stylesheet': 15,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 3.3962264150943398,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 13, 40, 43, 409471, tzinfo=datetime.timezone.utc)}
2025-09-12 18:41:36 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 18:41:36 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:41:36 [scrapy-playwright] INFO: Closing download handler
2025-09-12 18:41:36 [scrapy-playwright] INFO: Closing browser
2025-09-12 19:10:37 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 19:10:37 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 19:10:37 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 19:10:37 [scrapy.extensions.telnet] INFO: Telnet Password: acdb3fb008922b60
2025-09-12 19:10:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 19:10:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 19:10:38 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 19:10:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 19:10:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 19:10:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 19:10:38 [scrapy.core.engine] INFO: Spider opened
2025-09-12 19:10:38 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 19:10:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 19:10:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 19:10:38 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:10:38 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:10:40 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 19:10:41 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 19:11:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 19:11:36 [scrapy.extensions.feedexport] INFO: Stored csv feed (2 items) in: data/prod5-6-w-selenium.csv
2025-09-12 19:11:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157564,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 58.240072,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 14, 11, 36, 906734, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 2.0689655172413794,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 359,
 'playwright/request_count/method/GET': 305,
 'playwright/request_count/method/POST': 54,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 10,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 41,
 'playwright/request_count/resource_type/script': 127,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 345,
 'playwright/response_count/method/GET': 298,
 'playwright/response_count/method/POST': 47,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 10,
 'playwright/response_count/resource_type/font': 12,
 'playwright/response_count/resource_type/image': 119,
 'playwright/response_count/resource_type/ping': 34,
 'playwright/response_count/resource_type/script': 121,
 'playwright/response_count/resource_type/stylesheet': 16,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 3.103448275862069,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 14, 10, 38, 666662, tzinfo=datetime.timezone.utc)}
2025-09-12 19:11:36 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 19:11:36 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:11:36 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:11:36 [scrapy-playwright] INFO: Closing browser
2025-09-12 19:32:36 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 19:32:36 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 19:32:36 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 19:32:36 [scrapy.extensions.telnet] INFO: Telnet Password: e28a1e1b1dc53ea5
2025-09-12 19:32:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 19:32:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 19:32:37 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 19:32:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 19:32:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 19:32:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 19:32:37 [scrapy.core.engine] INFO: Spider opened
2025-09-12 19:32:37 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 19:32:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 19:32:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 19:32:37 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:32:37 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:32:38 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 19:32:39 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 19:33:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-09-12 19:33:42 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 19:33:42 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-7-w-selenium.json
2025-09-12 19:33:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157532,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 65.746144,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 14, 33, 42, 901333, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 1.8461538461538463,
 'log_count/INFO': 17,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 351,
 'playwright/request_count/method/GET': 304,
 'playwright/request_count/method/POST': 47,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 10,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 34,
 'playwright/request_count/resource_type/script': 126,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 335,
 'playwright/response_count/method/GET': 296,
 'playwright/response_count/method/POST': 39,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 10,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 120,
 'playwright/response_count/resource_type/ping': 26,
 'playwright/response_count/resource_type/script': 119,
 'playwright/response_count/resource_type/stylesheet': 16,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 2.7692307692307696,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 14, 32, 37, 155189, tzinfo=datetime.timezone.utc)}
2025-09-12 19:33:42 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 19:33:42 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:33:42 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:33:42 [scrapy-playwright] INFO: Closing browser
2025-09-12 19:34:23 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 19:34:23 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 19:34:23 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 19:34:23 [scrapy.extensions.telnet] INFO: Telnet Password: 08947926be57b712
2025-09-12 19:34:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 19:34:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 19:34:24 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 19:34:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 19:34:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 19:34:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 19:34:24 [scrapy.core.engine] INFO: Spider opened
2025-09-12 19:34:24 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 19:34:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 19:34:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 19:34:24 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:34:24 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:34:26 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 19:34:26 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 19:35:16 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 19:35:16 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/prod5-7-w-selenium.json
2025-09-12 19:35:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157772,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 51.868732,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 14, 35, 16, 619418, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 2.3529411764705883,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 357,
 'playwright/request_count/method/GET': 308,
 'playwright/request_count/method/POST': 49,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 10,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 120,
 'playwright/request_count/resource_type/ping': 37,
 'playwright/request_count/resource_type/script': 128,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 21,
 'playwright/response_count': 333,
 'playwright/response_count/method/GET': 296,
 'playwright/response_count/method/POST': 37,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 10,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 117,
 'playwright/response_count/resource_type/ping': 25,
 'playwright/response_count/resource_type/script': 121,
 'playwright/response_count/resource_type/stylesheet': 15,
 'playwright/response_count/resource_type/xhr': 21,
 'response_received_count': 3,
 'responses_per_minute': 3.5294117647058822,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 14, 34, 24, 750686, tzinfo=datetime.timezone.utc)}
2025-09-12 19:35:16 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 19:35:16 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:35:16 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:35:16 [scrapy-playwright] INFO: Closing browser
2025-09-12 19:52:15 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 19:52:15 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 19:52:15 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 19:52:15 [scrapy.extensions.telnet] INFO: Telnet Password: cdeaa2b77c5859dd
2025-09-12 19:52:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 19:52:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 19:52:16 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 19:52:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 19:52:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 19:52:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 19:52:16 [scrapy.core.engine] INFO: Spider opened
2025-09-12 19:52:16 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 19:52:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 19:52:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 19:52:16 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:52:16 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:52:18 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 19:52:18 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 19:52:50 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 19:52:50 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/f.json
2025-09-12 19:52:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157747,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 34.073928,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 14, 52, 50, 485366, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 3.5294117647058822,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 348,
 'playwright/request_count/method/GET': 306,
 'playwright/request_count/method/POST': 42,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 13,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 119,
 'playwright/request_count/resource_type/ping': 28,
 'playwright/request_count/resource_type/script': 127,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 330,
 'playwright/response_count/method/GET': 296,
 'playwright/response_count/method/POST': 34,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 13,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 119,
 'playwright/response_count/resource_type/ping': 20,
 'playwright/response_count/resource_type/script': 120,
 'playwright/response_count/resource_type/stylesheet': 14,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 5.294117647058823,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 14, 52, 16, 411438, tzinfo=datetime.timezone.utc)}
2025-09-12 19:52:50 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 19:52:50 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:52:50 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:52:50 [scrapy-playwright] INFO: Closing browser
2025-09-12 19:56:44 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 19:56:44 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 19:56:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 19:56:44 [scrapy.extensions.telnet] INFO: Telnet Password: d508c75309a9b8df
2025-09-12 19:56:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 19:56:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 19:56:44 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 19:56:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 19:56:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 19:56:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 19:56:44 [scrapy.core.engine] INFO: Spider opened
2025-09-12 19:56:44 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 19:56:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 19:56:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 19:56:44 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:56:44 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:56:46 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 19:56:46 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 19:57:29 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 19:57:29 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: data/f.json
2025-09-12 19:57:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 1157739,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 44.37794,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 14, 57, 29, 265941, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 2,
 'items_per_minute': 2.7272727272727275,
 'log_count/INFO': 16,
 'log_count/WARNING': 1,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 354,
 'playwright/request_count/method/GET': 307,
 'playwright/request_count/method/POST': 47,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 13,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 119,
 'playwright/request_count/resource_type/ping': 33,
 'playwright/request_count/resource_type/script': 128,
 'playwright/request_count/resource_type/stylesheet': 16,
 'playwright/request_count/resource_type/xhr': 20,
 'playwright/response_count': 324,
 'playwright/response_count/method/GET': 288,
 'playwright/response_count/method/POST': 36,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 13,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 110,
 'playwright/response_count/resource_type/ping': 22,
 'playwright/response_count/resource_type/script': 121,
 'playwright/response_count/resource_type/stylesheet': 14,
 'playwright/response_count/resource_type/xhr': 20,
 'response_received_count': 3,
 'responses_per_minute': 4.090909090909091,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 14, 56, 44, 888001, tzinfo=datetime.timezone.utc)}
2025-09-12 19:57:29 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 19:57:29 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:57:29 [scrapy-playwright] INFO: Closing download handler
2025-09-12 19:57:29 [scrapy-playwright] INFO: Closing browser
2025-09-12 19:58:14 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 19:58:14 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 19:58:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 19:58:14 [scrapy.extensions.telnet] INFO: Telnet Password: bea7e27a17047dad
2025-09-12 19:58:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 19:58:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 19:58:15 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 19:58:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 19:58:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 19:58:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 19:58:15 [scrapy.core.engine] INFO: Spider opened
2025-09-12 19:58:15 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 19:58:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 19:58:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 19:58:15 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:58:15 [scrapy-playwright] INFO: Starting download handler
2025-09-12 19:58:17 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 19:58:17 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 19:59:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 20:00:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 20:00:17 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i134926133-s1295236776.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134926133-s1295236776.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134926133-s1295236776.html", waiting until "load"

2025-09-12 20:00:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i134926133-s1295236776.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134926133-s1295236776.html", waiting until "load"

2025-09-12 20:00:32 [scrapy.core.engine] INFO: Closing spider (finished)
2025-09-12 20:00:32 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/f0.json
2025-09-12 20:00:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/playwright._impl._errors.TimeoutError': 1,
 'downloader/request_bytes': 703,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 580519,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 136.82529,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 9, 12, 15, 0, 32, 88606, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 567,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': 0.4411764705882353,
 'log_count/ERROR': 1,
 'log_count/INFO': 18,
 'log_count/WARNING': 2,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 2,
 'playwright/page_count/closed': 2,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 352,
 'playwright/request_count/method/GET': 298,
 'playwright/request_count/method/POST': 54,
 'playwright/request_count/navigation': 13,
 'playwright/request_count/resource_type/document': 13,
 'playwright/request_count/resource_type/fetch': 11,
 'playwright/request_count/resource_type/font': 12,
 'playwright/request_count/resource_type/image': 118,
 'playwright/request_count/resource_type/ping': 38,
 'playwright/request_count/resource_type/script': 123,
 'playwright/request_count/resource_type/stylesheet': 15,
 'playwright/request_count/resource_type/xhr': 22,
 'playwright/response_count': 338,
 'playwright/response_count/method/GET': 289,
 'playwright/response_count/method/POST': 49,
 'playwright/response_count/resource_type/document': 13,
 'playwright/response_count/resource_type/fetch': 11,
 'playwright/response_count/resource_type/font': 11,
 'playwright/response_count/resource_type/image': 114,
 'playwright/response_count/resource_type/ping': 33,
 'playwright/response_count/resource_type/script': 119,
 'playwright/response_count/resource_type/stylesheet': 15,
 'playwright/response_count/resource_type/xhr': 22,
 'response_received_count': 2,
 'responses_per_minute': 0.8823529411764706,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 9, 12, 14, 58, 15, 263316, tzinfo=datetime.timezone.utc)}
2025-09-12 20:00:32 [scrapy.core.engine] INFO: Spider closed (finished)
2025-09-12 20:00:32 [scrapy-playwright] INFO: Closing download handler
2025-09-12 20:00:32 [scrapy-playwright] INFO: Closing download handler
2025-09-12 20:00:32 [scrapy-playwright] INFO: Closing browser
2025-09-12 20:02:32 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: daraz)
2025-09-12 20:02:32 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.1',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.2 5 Aug 2025)',
 'cryptography': '45.0.7',
 'Platform': 'Windows-10-10.0.19045-SP0'}
2025-09-12 20:02:32 [scrapy.addons] INFO: Enabled addons:
[]
2025-09-12 20:02:32 [scrapy.extensions.telnet] INFO: Telnet Password: 515d7469cc6fe038
2025-09-12 20:02:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-09-12 20:02:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'daraz',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'spiderinfo.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'daraz.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['daraz.spiders']}
2025-09-12 20:02:33 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-09-12 20:02:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-09-12 20:02:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-09-12 20:02:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-09-12 20:02:33 [scrapy.core.engine] INFO: Spider opened
2025-09-12 20:02:33 [py.warnings] WARNING: D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py:433: ScrapyDeprecationWarning: daraz.spiders.products.ProductsSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2025-09-12 20:02:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 20:02:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-09-12 20:02:33 [scrapy-playwright] INFO: Starting download handler
2025-09-12 20:02:33 [scrapy-playwright] INFO: Starting download handler
2025-09-12 20:02:35 [scrapy-playwright] INFO: Launching browser chromium
2025-09-12 20:02:35 [scrapy-playwright] INFO: Browser chromium launched
2025-09-12 20:03:33 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-09-12 20:04:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-09-12 20:05:33 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 2 pages/min), scraped 3 items (at 2 items/min)
2025-09-12 20:06:33 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2025-09-12 20:07:29 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i134869708-s1295172171.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134869708-s1295172171.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134869708-s1295172171.html", waiting until "load"

2025-09-12 20:07:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i134869708-s1295172171.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134869708-s1295172171.html", waiting until "load"

2025-09-12 20:07:33 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2025-09-12 20:08:33 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2025-09-12 20:09:33 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 2 pages/min), scraped 5 items (at 2 items/min)
2025-09-12 20:10:33 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 4 pages/min), scraped 9 items (at 4 items/min)
2025-09-12 20:11:33 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 2 pages/min), scraped 11 items (at 2 items/min)
2025-09-12 20:12:33 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 3 pages/min), scraped 14 items (at 3 items/min)
2025-09-12 20:13:33 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 4 pages/min), scraped 18 items (at 4 items/min)
2025-09-12 20:14:33 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 5 pages/min), scraped 23 items (at 5 items/min)
2025-09-12 20:15:33 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 4 pages/min), scraped 27 items (at 4 items/min)
2025-09-12 20:16:33 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 0 pages/min), scraped 27 items (at 0 items/min)
2025-09-12 20:17:28 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i133954991-s1449487547.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i133954991-s1449487547.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i133954991-s1449487547.html", waiting until "load"

2025-09-12 20:17:28 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i133954991-s1449487547.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i133954991-s1449487547.html", waiting until "load"

2025-09-12 20:17:33 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 0 pages/min), scraped 27 items (at 0 items/min)
2025-09-12 20:18:33 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 0 pages/min), scraped 27 items (at 0 items/min)
2025-09-12 20:19:33 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 2 pages/min), scraped 29 items (at 2 items/min)
2025-09-12 20:20:33 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 0 pages/min), scraped 29 items (at 0 items/min)
2025-09-12 20:20:55 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i134117590-s1294489616.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134117590-s1294489616.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134117590-s1294489616.html", waiting until "load"

2025-09-12 20:20:55 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i134117590-s1294489616.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i134117590-s1294489616.html", waiting until "load"

2025-09-12 20:21:33 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 0 pages/min), scraped 29 items (at 0 items/min)
2025-09-12 20:22:33 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 1 pages/min), scraped 30 items (at 1 items/min)
2025-09-12 20:23:33 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 0 pages/min), scraped 30 items (at 0 items/min)
2025-09-12 20:23:42 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i135143887-s1295460858.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135143887-s1295460858.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135143887-s1295460858.html", waiting until "load"

2025-09-12 20:23:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i135143887-s1295460858.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135143887-s1295460858.html", waiting until "load"

2025-09-12 20:24:33 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 0 pages/min), scraped 30 items (at 0 items/min)
2025-09-12 20:25:33 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 0 pages/min), scraped 30 items (at 0 items/min)
2025-09-12 20:25:42 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i135191487-s1295497883.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135191487-s1295497883.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135191487-s1295497883.html", waiting until "load"

2025-09-12 20:25:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i135191487-s1295497883.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135191487-s1295497883.html", waiting until "load"

2025-09-12 20:26:33 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 0 pages/min), scraped 30 items (at 0 items/min)
2025-09-12 20:27:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 1 pages/min), scraped 31 items (at 1 items/min)
2025-09-12 20:28:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:29:13 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i135720900-s1296080563.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135720900-s1296080563.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135720900-s1296080563.html", waiting until "load"

2025-09-12 20:29:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i135720900-s1296080563.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135720900-s1296080563.html", waiting until "load"

2025-09-12 20:29:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:30:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:31:14 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i135798983-s1296160115.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135798983-s1296160115.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135798983-s1296160115.html", waiting until "load"

2025-09-12 20:31:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i135798983-s1296160115.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135798983-s1296160115.html", waiting until "load"

2025-09-12 20:31:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:32:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:33:14 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i135832986-s1296190385.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135832986-s1296190385.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135832986-s1296190385.html", waiting until "load"

2025-09-12 20:33:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i135832986-s1296190385.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i135832986-s1296190385.html", waiting until "load"

2025-09-12 20:33:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:34:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:35:14 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i136088208-s1296400630.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i136088208-s1296400630.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i136088208-s1296400630.html", waiting until "load"

2025-09-12 20:35:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i136088208-s1296400630.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i136088208-s1296400630.html", waiting until "load"

2025-09-12 20:35:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:36:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:37:14 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i136170792-s1296494048.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i136170792-s1296494048.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i136170792-s1296494048.html", waiting until "load"

2025-09-12 20:37:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i136170792-s1296494048.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i136170792-s1296494048.html", waiting until "load"

2025-09-12 20:37:33 [scrapy.extensions.logstats] INFO: Crawled 32 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-09-12 20:38:33 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 2 pages/min), scraped 33 items (at 2 items/min)
2025-09-12 20:39:33 [scrapy.extensions.logstats] INFO: Crawled 35 pages (at 1 pages/min), scraped 34 items (at 1 items/min)
2025-09-12 20:40:33 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 2 pages/min), scraped 36 items (at 2 items/min)
2025-09-12 20:41:33 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 1 pages/min), scraped 37 items (at 1 items/min)
2025-09-12 20:42:33 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 0 pages/min), scraped 37 items (at 0 items/min)
2025-09-12 20:43:22 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i139700622-s1299686391.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i139700622-s1299686391.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i139700622-s1299686391.html", waiting until "load"

2025-09-12 20:43:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i139700622-s1299686391.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i139700622-s1299686391.html", waiting until "load"

2025-09-12 20:43:33 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 0 pages/min), scraped 37 items (at 0 items/min)
2025-09-12 20:44:33 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 0 pages/min), scraped 37 items (at 0 items/min)
2025-09-12 20:45:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.daraz.pk/i140500677-s1300522959.html> (referer: None)
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\utils\defer.py", line 343, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\utils\python.py", line 369, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\utils\python.py", line 369, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 59, in process_spider_output
    yield from super().process_spider_output(response, result, spider)
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\daraz\daraz\spiders\products.py", line 52, in parse
    seller_info_value = response.css(".seller-info-value::text").getall()
                       ^^^^^^^^^^^^^^^^^^^^
IndexError: list index out of range
2025-09-12 20:45:33 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 1 pages/min), scraped 37 items (at 0 items/min)
2025-09-12 20:45:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.daraz.pk/i140506692-s1300538787.html> (referer: None)
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\utils\defer.py", line 343, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\utils\python.py", line 369, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\utils\python.py", line 369, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 59, in process_spider_output
    yield from super().process_spider_output(response, result, spider)
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\spidermiddlewares\base.py", line 58, in process_spider_output
    for o in result:
             ^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\spidermw.py", line 167, in process_sync
    yield from iterable
  File "D:\ecommerce-analytics\python-scrape\daraz\daraz\spiders\products.py", line 52, in parse
    seller_info_value = response.css(".seller-info-value::text").getall()
                       ^^^^^^^^^^^^^^^^^^^^
IndexError: list index out of range
2025-09-12 20:46:33 [scrapy.extensions.logstats] INFO: Crawled 41 pages (at 2 pages/min), scraped 38 items (at 1 items/min)
2025-09-12 20:47:33 [scrapy.extensions.logstats] INFO: Crawled 41 pages (at 0 pages/min), scraped 38 items (at 0 items/min)
2025-09-12 20:48:33 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 3 pages/min), scraped 41 items (at 3 items/min)
2025-09-12 20:49:33 [scrapy.extensions.logstats] INFO: Crawled 47 pages (at 3 pages/min), scraped 44 items (at 3 items/min)
2025-09-12 20:50:33 [scrapy.extensions.logstats] INFO: Crawled 51 pages (at 4 pages/min), scraped 48 items (at 4 items/min)
2025-09-12 20:51:33 [scrapy.extensions.logstats] INFO: Crawled 55 pages (at 4 pages/min), scraped 52 items (at 4 items/min)
2025-09-12 20:52:33 [scrapy.extensions.logstats] INFO: Crawled 58 pages (at 3 pages/min), scraped 55 items (at 3 items/min)
2025-09-12 20:53:33 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 2 pages/min), scraped 57 items (at 2 items/min)
2025-09-12 20:54:33 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 57 items (at 0 items/min)
2025-09-12 20:55:32 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i141594580-s1301634648.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i141594580-s1301634648.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i141594580-s1301634648.html", waiting until "load"

2025-09-12 20:55:32 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i141594580-s1301634648.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i141594580-s1301634648.html", waiting until "load"

2025-09-12 20:55:33 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 57 items (at 0 items/min)
2025-09-12 20:56:33 [scrapy.extensions.logstats] INFO: Crawled 61 pages (at 1 pages/min), scraped 58 items (at 1 items/min)
2025-09-12 20:57:33 [scrapy.extensions.logstats] INFO: Crawled 62 pages (at 1 pages/min), scraped 59 items (at 1 items/min)
2025-09-12 20:57:56 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i143078168-s1303888462.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i143078168-s1303888462.html
Call log:
  - navigating to "https://www.daraz.pk/i143078168-s1303888462.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i143078168-s1303888462.html
Call log:
  - navigating to "https://www.daraz.pk/i143078168-s1303888462.html", waiting until "load"

2025-09-12 20:57:56 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144664889-s1306256065.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144664889-s1306256065.html
Call log:
  - navigating to "https://www.daraz.pk/i144664889-s1306256065.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144664889-s1306256065.html
Call log:
  - navigating to "https://www.daraz.pk/i144664889-s1306256065.html", waiting until "load"

2025-09-12 20:57:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i143078168-s1303888462.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i143078168-s1303888462.html
Call log:
  - navigating to "https://www.daraz.pk/i143078168-s1303888462.html", waiting until "load"

2025-09-12 20:57:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144664889-s1306256065.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144664889-s1306256065.html
Call log:
  - navigating to "https://www.daraz.pk/i144664889-s1306256065.html", waiting until "load"

2025-09-12 20:57:57 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144666858-s1306248388.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144666858-s1306248388.html
Call log:
  - navigating to "https://www.daraz.pk/i144666858-s1306248388.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144666858-s1306248388.html
Call log:
  - navigating to "https://www.daraz.pk/i144666858-s1306248388.html", waiting until "load"

2025-09-12 20:57:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144666858-s1306248388.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144666858-s1306248388.html
Call log:
  - navigating to "https://www.daraz.pk/i144666858-s1306248388.html", waiting until "load"

2025-09-12 20:57:57 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144676022-s1306232705.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144676022-s1306232705.html
Call log:
  - navigating to "https://www.daraz.pk/i144676022-s1306232705.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144676022-s1306232705.html
Call log:
  - navigating to "https://www.daraz.pk/i144676022-s1306232705.html", waiting until "load"

2025-09-12 20:57:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144676022-s1306232705.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144676022-s1306232705.html
Call log:
  - navigating to "https://www.daraz.pk/i144676022-s1306232705.html", waiting until "load"

2025-09-12 20:57:57 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144684123-s1306240679.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144684123-s1306240679.html
Call log:
  - navigating to "https://www.daraz.pk/i144684123-s1306240679.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144684123-s1306240679.html
Call log:
  - navigating to "https://www.daraz.pk/i144684123-s1306240679.html", waiting until "load"

2025-09-12 20:57:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144684123-s1306240679.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144684123-s1306240679.html
Call log:
  - navigating to "https://www.daraz.pk/i144684123-s1306240679.html", waiting until "load"

2025-09-12 20:57:58 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144686509-s1306264504.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144686509-s1306264504.html
Call log:
  - navigating to "https://www.daraz.pk/i144686509-s1306264504.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144686509-s1306264504.html
Call log:
  - navigating to "https://www.daraz.pk/i144686509-s1306264504.html", waiting until "load"

2025-09-12 20:57:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144686509-s1306264504.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i144686509-s1306264504.html
Call log:
  - navigating to "https://www.daraz.pk/i144686509-s1306264504.html", waiting until "load"

2025-09-12 21:18:00 [scrapy.extensions.logstats] INFO: Crawled 64 pages (at 2 pages/min), scraped 61 items (at 2 items/min)
2025-09-12 21:18:03 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i133889035-s1294274257.html> exc_type=<class 'playwright._impl._errors.Error'> exc_msg=Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i133889035-s1294274257.html
Call log:
  - navigating to "https://www.daraz.pk/i133889035-s1294274257.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i133889035-s1294274257.html
Call log:
  - navigating to "https://www.daraz.pk/i133889035-s1294274257.html", waiting until "load"

2025-09-12 21:18:04 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i133889035-s1294274257.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: Page.goto: net::ERR_NETWORK_IO_SUSPENDED at https://www.daraz.pk/i133889035-s1294274257.html
Call log:
  - navigating to "https://www.daraz.pk/i133889035-s1294274257.html", waiting until "load"

2025-09-12 21:18:33 [scrapy.extensions.logstats] INFO: Crawled 64 pages (at 0 pages/min), scraped 61 items (at 0 items/min)
2025-09-12 21:19:33 [scrapy.extensions.logstats] INFO: Crawled 65 pages (at 1 pages/min), scraped 62 items (at 1 items/min)
2025-09-12 21:20:33 [scrapy.extensions.logstats] INFO: Crawled 65 pages (at 0 pages/min), scraped 62 items (at 0 items/min)
2025-09-12 21:21:33 [scrapy.extensions.logstats] INFO: Crawled 66 pages (at 1 pages/min), scraped 63 items (at 1 items/min)
2025-09-12 21:22:33 [scrapy.extensions.logstats] INFO: Crawled 67 pages (at 1 pages/min), scraped 64 items (at 1 items/min)
2025-09-12 21:23:33 [scrapy.extensions.logstats] INFO: Crawled 71 pages (at 4 pages/min), scraped 68 items (at 4 items/min)
2025-09-12 21:24:33 [scrapy.extensions.logstats] INFO: Crawled 75 pages (at 4 pages/min), scraped 72 items (at 4 items/min)
2025-09-12 21:25:33 [scrapy.extensions.logstats] INFO: Crawled 79 pages (at 4 pages/min), scraped 76 items (at 4 items/min)
2025-09-12 21:26:33 [scrapy.extensions.logstats] INFO: Crawled 83 pages (at 4 pages/min), scraped 80 items (at 4 items/min)
2025-09-12 21:27:33 [scrapy.extensions.logstats] INFO: Crawled 87 pages (at 4 pages/min), scraped 84 items (at 4 items/min)
2025-09-12 21:28:33 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 3 pages/min), scraped 87 items (at 3 items/min)
2025-09-12 21:29:33 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 0 pages/min), scraped 87 items (at 0 items/min)
2025-09-12 21:30:33 [scrapy.extensions.logstats] INFO: Crawled 92 pages (at 2 pages/min), scraped 89 items (at 2 items/min)
2025-09-12 21:31:33 [scrapy.extensions.logstats] INFO: Crawled 92 pages (at 0 pages/min), scraped 89 items (at 0 items/min)
2025-09-12 21:32:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 1 pages/min), scraped 90 items (at 1 items/min)
2025-09-12 21:33:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2025-09-12 21:34:06 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144644598-s1306218557.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144644598-s1306218557.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144644598-s1306218557.html", waiting until "load"

2025-09-12 21:34:06 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144644598-s1306218557.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144644598-s1306218557.html", waiting until "load"

2025-09-12 21:34:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2025-09-12 21:35:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2025-09-12 21:36:06 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144644627-s1306218596.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144644627-s1306218596.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144644627-s1306218596.html", waiting until "load"

2025-09-12 21:36:06 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144644627-s1306218596.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144644627-s1306218596.html", waiting until "load"

2025-09-12 21:36:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2025-09-12 21:37:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2025-09-12 21:38:06 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144658415-s1306228306.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144658415-s1306228306.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144658415-s1306228306.html", waiting until "load"

2025-09-12 21:38:06 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144658415-s1306228306.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144658415-s1306228306.html", waiting until "load"

2025-09-12 21:38:33 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2025-09-12 21:39:33 [scrapy.extensions.logstats] INFO: Crawled 94 pages (at 1 pages/min), scraped 91 items (at 1 items/min)
2025-09-12 21:40:33 [scrapy.extensions.logstats] INFO: Crawled 95 pages (at 1 pages/min), scraped 92 items (at 1 items/min)
2025-09-12 21:41:33 [scrapy.extensions.logstats] INFO: Crawled 95 pages (at 0 pages/min), scraped 92 items (at 0 items/min)
2025-09-12 21:41:48 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144664350-s1306226386.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144664350-s1306226386.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144664350-s1306226386.html", waiting until "load"

2025-09-12 21:41:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144664350-s1306226386.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144664350-s1306226386.html", waiting until "load"

2025-09-12 21:42:33 [scrapy.extensions.logstats] INFO: Crawled 96 pages (at 1 pages/min), scraped 93 items (at 1 items/min)
2025-09-12 21:43:33 [scrapy.extensions.logstats] INFO: Crawled 96 pages (at 0 pages/min), scraped 93 items (at 0 items/min)
2025-09-12 21:44:33 [scrapy.extensions.logstats] INFO: Crawled 97 pages (at 1 pages/min), scraped 94 items (at 1 items/min)
2025-09-12 21:45:33 [scrapy.extensions.logstats] INFO: Crawled 97 pages (at 0 pages/min), scraped 94 items (at 0 items/min)
2025-09-12 21:45:44 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.daraz.pk/i144672244-s1306238568.html> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144672244-s1306238568.html", waiting until "load"
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144672244-s1306238568.html", waiting until "load"

2025-09-12 21:45:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.daraz.pk/i144672244-s1306238568.html>
Traceback (most recent call last):
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\internet\defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\twisted\python\failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy\core\downloader\middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\_utils.py", line 123, in _handle_coro
    result = await coro
             ^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 380, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 462, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\scrapy_playwright\handler.py", line 564, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\async_api\_generated.py", line 8992, in goto
    await self._impl_obj.goto(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_page.py", line 556, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_frame.py", line 153, in goto
    await self._channel.send(
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 69, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ecommerce-analytics\python-scrape\env\Lib\site-packages\playwright\_impl\_connection.py", line 558, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 120000ms exceeded.
Call log:
  - navigating to "https://www.daraz.pk/i144672244-s1306238568.html", waiting until "load"

2025-09-12 21:46:33 [scrapy.extensions.logstats] INFO: Crawled 99 pages (at 2 pages/min), scraped 96 items (at 2 items/min)
2025-09-12 21:47:33 [scrapy.extensions.logstats] INFO: Crawled 99 pages (at 0 pages/min), scraped 96 items (at 0 items/min)
2025-09-12 21:48:33 [scrapy.extensions.logstats] INFO: Crawled 100 pages (at 1 pages/min), scraped 97 items (at 1 items/min)
2025-09-12 21:49:33 [scrapy.extensions.logstats] INFO: Crawled 103 pages (at 3 pages/min), scraped 100 items (at 3 items/min)
2025-09-12 21:50:33 [scrapy.extensions.logstats] INFO: Crawled 105 pages (at 2 pages/min), scraped 102 items (at 2 items/min)
2025-09-12 21:51:33 [scrapy.extensions.logstats] INFO: Crawled 107 pages (at 2 pages/min), scraped 104 items (at 2 items/min)
2025-09-12 21:52:33 [scrapy.extensions.logstats] INFO: Crawled 108 pages (at 1 pages/min), scraped 105 items (at 1 items/min)
2025-09-12 21:53:33 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 1 pages/min), scraped 106 items (at 1 items/min)
2025-09-12 21:54:33 [scrapy.extensions.logstats] INFO: Crawled 112 pages (at 3 pages/min), scraped 109 items (at 3 items/min)
2025-09-12 21:55:33 [scrapy.extensions.logstats] INFO: Crawled 113 pages (at 1 pages/min), scraped 110 items (at 1 items/min)
2025-09-12 21:56:33 [scrapy.extensions.logstats] INFO: Crawled 114 pages (at 1 pages/min), scraped 111 items (at 1 items/min)
2025-09-12 21:57:33 [scrapy.extensions.logstats] INFO: Crawled 116 pages (at 2 pages/min), scraped 113 items (at 2 items/min)
2025-09-12 21:58:33 [scrapy.extensions.logstats] INFO: Crawled 117 pages (at 1 pages/min), scraped 114 items (at 1 items/min)
2025-09-12 21:59:33 [scrapy.extensions.logstats] INFO: Crawled 117 pages (at 0 pages/min), scraped 114 items (at 0 items/min)
2025-09-12 22:00:33 [scrapy.extensions.logstats] INFO: Crawled 119 pages (at 2 pages/min), scraped 116 items (at 2 items/min)
2025-09-12 22:01:33 [scrapy.extensions.logstats] INFO: Crawled 122 pages (at 3 pages/min), scraped 119 items (at 3 items/min)
2025-09-12 22:02:33 [scrapy.extensions.logstats] INFO: Crawled 125 pages (at 3 pages/min), scraped 122 items (at 3 items/min)
2025-09-12 22:03:33 [scrapy.extensions.logstats] INFO: Crawled 128 pages (at 3 pages/min), scraped 125 items (at 3 items/min)
2025-09-12 22:04:33 [scrapy.extensions.logstats] INFO: Crawled 131 pages (at 3 pages/min), scraped 128 items (at 3 items/min)
2025-09-12 22:05:33 [scrapy.extensions.logstats] INFO: Crawled 133 pages (at 2 pages/min), scraped 130 items (at 2 items/min)
2025-09-12 22:06:33 [scrapy.extensions.logstats] INFO: Crawled 137 pages (at 4 pages/min), scraped 134 items (at 4 items/min)
2025-09-12 22:07:33 [scrapy.extensions.logstats] INFO: Crawled 139 pages (at 2 pages/min), scraped 136 items (at 2 items/min)
2025-09-12 22:08:33 [scrapy.extensions.logstats] INFO: Crawled 142 pages (at 3 pages/min), scraped 139 items (at 3 items/min)
2025-09-12 22:09:33 [scrapy.extensions.logstats] INFO: Crawled 145 pages (at 3 pages/min), scraped 142 items (at 3 items/min)
2025-09-12 22:10:33 [scrapy.extensions.logstats] INFO: Crawled 148 pages (at 3 pages/min), scraped 145 items (at 3 items/min)
2025-09-12 22:11:33 [scrapy.extensions.logstats] INFO: Crawled 149 pages (at 1 pages/min), scraped 146 items (at 1 items/min)
2025-09-12 22:12:33 [scrapy.extensions.logstats] INFO: Crawled 150 pages (at 1 pages/min), scraped 147 items (at 1 items/min)
